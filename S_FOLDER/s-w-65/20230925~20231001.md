## 내용

1. 모노레포 적용하면서 이해가 된 부분들 정리해보자.
2. 모노레포는 우선 `package`의 의존성을 관리해주진 않는다. workspace의 의존성을 관리할 때 필요하다.
3. 즉, 와플 코어를 쪼개서 정리를 하고, 해당 유틸리티 등이 필요한데 의존성이 굉장히 많아서 하나의 레포에서 작업을 하고 싶을 때 사용하는 것이 좋다.
4. 물론 pnpm을 사용해서 공용으로 사용하는 `pacakge`의 `node_modules`는 함께 사용할 수 있다.
5. 하지만, `package.json`에는 전부 따로 작성해줘야 한다.
6. 주된 목적은 그런 것 같음. `detect` 옵션으로 버전을 관리해줘도 될 것 같다.

## 에러

1. pnpm install 할 때의 에러.
   - type 지정이 안되어 있었음. mui 같은 경우 찾지도 못하고 있었음.
   - 우선적으로 `React.ElementType`으로 감싸서 해결. Mui의 경우 잘 모르기에.
2. monorepo로 전환하며 배운 점
   - 의존성 변경의 주의점
   - monolithic에서 monorepo로 갈 경우 대비를 톡톡히 해둬야 한다.
   - 바뀔 곳이 아주 많으며, 공통부분에서 `import`할 경우 변경이 난해해진다.

---

- 아래는 그냥 어레이버퍼 사용법 관련

```js
import { EncodingParams, FileMeta } from "./types";
import {
  getFileMetaList,
  getFilesFromArrayBuffer,
  merge,
  readAsArrayBuffer,
} from "./util";

export async function encode({
  fixedHeader,
  variableHeader,
  variableBody,
  files,
}: EncodingParams) {
  const textEncoder = new TextEncoder();
  // 가변 헤더
  const variableHeaderBuffer = textEncoder.encode(
    JSON.stringify(variableHeader)
  );

  // 가변 바디
  const fileMetaList = getFileMetaList(files);
  const variableBodyBuffer = textEncoder.encode(
    JSON.stringify({
      ...variableBody,
      fileMetaList,
    })
  );

  // 바이너리
  const fileBuffers = await Promise.all(
    files.map((file) => readAsArrayBuffer(file))
  );
  const binaryBuffer = merge(...fileBuffers);

  // 고정 헤더
  const fixedHeaderBuffer = new ArrayBuffer(64);
  const dataview = new DataView(fixedHeaderBuffer);
  dataview.setUint16(0, fixedHeader.magicNumber);
  dataview.setUint16(2, fixedHeader.version);
  dataview.setUint16(4, fixedHeader.type.charCodeAt(0));
  dataview.setUint8(6, fixedHeader?.subProtocol);
  dataview.setUint32(16, fixedHeader.requestId);
  dataview.setUint32(24, variableHeaderBuffer.byteLength);
  dataview.setUint32(28, variableBodyBuffer.byteLength);
  dataview.setUint32(32, binaryBuffer.byteLength);
  dataview.setUint8(36, fixedHeader?.personaId);
  dataview.setUint32(40, fixedHeader?.userId);

  return merge(
    fixedHeaderBuffer,
    variableHeaderBuffer,
    variableBodyBuffer,
    binaryBuffer
  );
}

export function decode<H = any, B extends { fileMetaList: FileMeta[] } = any>(
  buffer: ArrayBuffer
) {
  const dataview = new DataView(buffer.slice(0, 64));

  const fixedHeader = {
    magicNumber: dataview.getUint16(0),
    version: dataview.getUint16(2),
    type: String.fromCharCode(dataview.getUint16(4)),
    subProtocol: dataview.getUint8(6),
    nodeId: dataview.getUint32(8),
    cliendId: dataview.getUint32(12),
    requestId: dataview.getUint32(16),
    headerLength: dataview.getUint32(24),
    bodyLength: dataview.getUint32(28),
    binaryLength: dataview.getUint32(32),
    personaId: dataview.getUint8(36),
    userId: dataview.getUint32(40),
  };

  const textDecoder = new TextDecoder();
  let offset = 64;

  const header: H = JSON.parse(
    textDecoder.decode(
      buffer.slice(offset, (offset += fixedHeader.headerLength))
    )
  );

  let body: B = null;
  if (fixedHeader.bodyLength > 0)
    body = JSON.parse(
      textDecoder.decode(
        buffer.slice(offset, (offset += fixedHeader.bodyLength))
      )
    );

  let files: File[] = [];
  if (body?.fileMetaList) {
    files = getFilesFromArrayBuffer(
      buffer.slice(offset, (offset += fixedHeader.binaryLength)),
      body.fileMetaList
    );
  }

  return { fixedHeader, header, body, files };
}
```

```js
import { FileMeta } from "./types";

export function merge(...buffers: ArrayBuffer[]) {
  const size = buffers.reduce(
    (acc, buffer) => acc + (buffer?.byteLength ?? 0),
    0
  );
  const arrayBuffer = new ArrayBuffer(size);
  const view = new Uint8Array(arrayBuffer);

  let offset = 0;
  buffers.forEach((buffer) => {
    view.set(new Uint8Array(buffer), offset);
    offset += buffer?.byteLength ?? 0;
  });

  return arrayBuffer;
}

export function readAsArrayBuffer(file: File): Promise<ArrayBuffer> {
  return new Promise((resolve, reject) => {
    const fileReader = new FileReader();
    fileReader.onload = () => {
      resolve(fileReader.result as ArrayBuffer);
    };

    fileReader.onerror = () => {
      reject(null);
    };

    fileReader.readAsArrayBuffer(file);
  });
}

export function getFileMetaList(files: File[]): FileMeta[] {
  return (
    files?.map(({ name, lastModified, size, type }) => ({
      name,
      lastModified,
      size,
      type,
    })) ?? []
  );
}

export function getFilesFromArrayBuffer(
  arrayBuffer: ArrayBuffer,
  fileMetaList: FileMeta[]
) {
  const files = [];
  const length = fileMetaList?.length ?? 0;
  let offset = 0;
  for (let i = 0; i < length; i++) {
    const buffer = arrayBuffer.slice(offset, offset + fileMetaList[i].size);
    const blob = new Blob([buffer]);
    const file = new File([blob], fileMetaList[i].name, { ...fileMetaList[i] });
    files.push(file);
  }
  return files;
}

```
